{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90qK_fohDCot",
        "outputId": "715d0d91-36f1-444d-8415-997227397d7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue Feb  8 16:37:18 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   62C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4VTVyE0F1rw"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rY4IPNsVF9vz"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oa1N2fgjGtKr",
        "outputId": "6063e907-493e-4143-c40d-cb7d899835ee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "import tensorflow as tf\n",
        "class GarbageCollectorCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHdtkkNH_lDW",
        "outputId": "b2553324-1753-44ec-de21-4ef18ba81798"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10000,) (10000,)\n",
            "5000\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "train_df = pd.read_csv('mami_training.csv', sep='\\t', lineterminator='\\r')\n",
        "test_df = pd.read_csv('Test.csv', sep='\\t', lineterminator='\\n')\n",
        "test_labels = pd.read_csv('test_labels.txt',header=None, sep='\\t', lineterminator='\\n')\n",
        "X = train_df['Text Transcription'].copy()\n",
        "for i in range(len(X)):\n",
        "  X[i] = X[i].lower()\n",
        "X_test_model = test_df['Text Transcription'].to_numpy()\n",
        "for i in range(len(X_test_model)):\n",
        "  X_test_model[i] = X_test_model[i].lower()\n",
        "Y = train_df['misogynous']\n",
        "X_train_model = X[:].to_numpy()\n",
        "Y_train_model = Y[:].to_numpy()\n",
        "print(X_train_model.shape, Y_train_model.shape)\n",
        "print(Y_train_model.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Q4I-zrk4JK6g"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 50\n",
        "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=MAX_LENGTH):\n",
        "    features = [] \n",
        "    for e in examples:\n",
        "        input_dict = tokenizer.encode_plus(\n",
        "            e[3],\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            return_token_type_ids=True,\n",
        "            pad_to_max_length=True,\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "        )\n",
        "\n",
        "        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],\n",
        "            input_dict[\"token_type_ids\"], input_dict['attention_mask'])\n",
        "\n",
        "        features.append(\n",
        "            InputFeatures(\n",
        "                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e[1]\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def gen():\n",
        "        for f in features:\n",
        "            yield (\n",
        "                {\n",
        "                    \"input_ids\": f.input_ids,\n",
        "                    \"attention_mask\": f.attention_mask,\n",
        "                    \"token_type_ids\": f.token_type_ids,\n",
        "                },\n",
        "                f.label,\n",
        "            )\n",
        "\n",
        "    return tf.data.Dataset.from_generator(\n",
        "        gen,\n",
        "        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n",
        "        (\n",
        "            {\n",
        "                \"input_ids\": tf.TensorShape([None]),\n",
        "                \"attention_mask\": tf.TensorShape([None]),\n",
        "                \"token_type_ids\": tf.TensorShape([None]),\n",
        "            },\n",
        "            tf.TensorShape([]),\n",
        "        ),\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JxHL3JuLHIJ-"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from transformers import InputExample, InputFeatures\n",
        "def try_model():\n",
        "  clf = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
        "  tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "  print(clf.summary())\n",
        "  clf.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-6, epsilon=1e-08, clipnorm=1.0), \n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "  train_bert = pd.DataFrame({\n",
        "  'guid': range(len(X_train_model)),\n",
        "  'label': Y_train_model,\n",
        "  'text_b': ['a']*len(X_train_model),\n",
        "  'text_a': X_train_model\n",
        "  })\n",
        "  bert_train, bert_val = train_test_split(train_bert, test_size = 0.1)\n",
        "  bert_train, bert_val = bert_train.to_numpy(), bert_val.to_numpy()\n",
        "  bert_train = convert_examples_to_tf_dataset(bert_train, tokenizer)\n",
        "  bert_train = bert_train.shuffle(2).batch(64).repeat(2)\n",
        "  bert_val = convert_examples_to_tf_dataset(bert_val, tokenizer)\n",
        "  bert_val = bert_val.shuffle(2).batch(64)\n",
        "  clf.fit(\n",
        "      bert_train, \n",
        "      epochs=5, \n",
        "      validation_data=bert_val, verbose = 2, \n",
        "      callbacks=[\n",
        "              tf.keras.callbacks.EarlyStopping(\n",
        "              monitor=\"val_accuracy\",\n",
        "              min_delta=0.005,\n",
        "              patience=4,\n",
        "              verbose=2,\n",
        "              mode=\"max\",\n",
        "              baseline=None,\n",
        "              restore_best_weights=True,\n",
        "              ),\n",
        "              GarbageCollectorCallback()])\n",
        "  tf_batch = tokenizer(list(X_test_model), max_length=MAX_LENGTH, padding=True, truncation=True, return_tensors='tf')\n",
        "  tf_outputs = clf(tf_batch)\n",
        "  label = tf.nn.softmax(tf_outputs[0], axis=-1)\n",
        "  label = np.array(tf.argmax(label, axis=1))\n",
        "  return label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeexSaGHPhuH",
        "outputId": "8c56ad79-c651-4fb0-a70f-ec46a9fc102c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"tf_bert_for_sequence_classification_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bert (TFBertMainLayer)      multiple                  109482240 \n",
            "                                                                 \n",
            " dropout_113 (Dropout)       multiple                  0         \n",
            "                                                                 \n",
            " classifier (Dense)          multiple                  1538      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 109,483,778\n",
            "Trainable params: 109,483,778\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2257: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "282/282 - 208s - loss: 0.5976 - accuracy: 0.6746 - val_loss: 0.4887 - val_accuracy: 0.7820 - 208s/epoch - 738ms/step\n",
            "Epoch 2/5\n",
            "282/282 - 183s - loss: 0.4537 - accuracy: 0.8004 - val_loss: 0.4491 - val_accuracy: 0.8160 - 183s/epoch - 651ms/step\n",
            "Epoch 3/5\n",
            "282/282 - 183s - loss: 0.3775 - accuracy: 0.8446 - val_loss: 0.4377 - val_accuracy: 0.8100 - 183s/epoch - 650ms/step\n",
            "Epoch 4/5\n",
            "282/282 - 182s - loss: 0.3218 - accuracy: 0.8726 - val_loss: 0.4412 - val_accuracy: 0.8210 - 182s/epoch - 646ms/step\n",
            "Epoch 5/5\n",
            "282/282 - 183s - loss: 0.2683 - accuracy: 0.9007 - val_loss: 0.4683 - val_accuracy: 0.8140 - 183s/epoch - 649ms/step\n",
            "######## Misogynous Prediction ########\n",
            "\n",
            "Submission Score \n",
            "[[260 240]\n",
            " [ 90 410]] \n",
            " F1 score - 0.6624040920716112\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "Y_test_model = test_labels[1].to_numpy()\n",
        "Y_train_model = train_df['misogynous'].to_numpy()\n",
        "print(\"######## Misogynous Prediction ########\")\n",
        "print(\"\\nSubmission Score \")\n",
        "y_pred = try_model()\n",
        "print(confusion_matrix(Y_test_model, y_pred),\"\\n F1 score - \", f1_score(Y_test_model, y_pred, average='macro'))\n",
        "subtaskA = f1_score(Y_test_model, y_pred, average='macro')\n",
        "subtaskB = f1_score(Y_test_model, y_pred, average='macro')*(Y_test_model.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoONDfsDbebh",
        "outputId": "e01197b9-8037-4fec-cb42-cd68869ab1ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"tf_bert_for_sequence_classification_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bert (TFBertMainLayer)      multiple                  109482240 \n",
            "                                                                 \n",
            " dropout_151 (Dropout)       multiple                  0         \n",
            "                                                                 \n",
            " classifier (Dense)          multiple                  1538      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 109,483,778\n",
            "Trainable params: 109,483,778\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2257: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "282/282 - 208s - loss: 0.3942 - accuracy: 0.8683 - val_loss: 0.3447 - val_accuracy: 0.8780 - 208s/epoch - 737ms/step\n",
            "Epoch 2/5\n",
            "282/282 - 183s - loss: 0.3248 - accuracy: 0.8742 - val_loss: 0.3226 - val_accuracy: 0.8870 - 183s/epoch - 650ms/step\n",
            "Epoch 3/5\n",
            "282/282 - 183s - loss: 0.2738 - accuracy: 0.8894 - val_loss: 0.3245 - val_accuracy: 0.8940 - 183s/epoch - 650ms/step\n",
            "Epoch 4/5\n",
            "282/282 - 183s - loss: 0.2274 - accuracy: 0.9087 - val_loss: 0.3478 - val_accuracy: 0.8730 - 183s/epoch - 648ms/step\n",
            "Epoch 5/5\n",
            "282/282 - 183s - loss: 0.1738 - accuracy: 0.9329 - val_loss: 0.3857 - val_accuracy: 0.8760 - 183s/epoch - 650ms/step\n",
            "\n",
            "######## Shaming Prediction ########\n",
            "[[791  63]\n",
            " [ 98  48]] \n",
            " F1 score -  0.6405856890597409\n"
          ]
        }
      ],
      "source": [
        "Y_train_model = train_df['shaming'].to_numpy()\n",
        "Y_test_model = test_labels[2].to_numpy()\n",
        "y_pred = try_model()\n",
        "print(\"\\n######## Shaming Prediction ########\")\n",
        "print(confusion_matrix(Y_test_model, y_pred),\"\\n F1 score - \", f1_score(Y_test_model, y_pred, average='macro'))\n",
        "subtaskB += f1_score(Y_test_model, y_pred, average='macro')*(Y_test_model.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjFZV3NocJYc",
        "outputId": "1c3c15d6-715d-4907-ed16-5797d3f40beb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"tf_bert_for_sequence_classification_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bert (TFBertMainLayer)      multiple                  109482240 \n",
            "                                                                 \n",
            " dropout_227 (Dropout)       multiple                  0         \n",
            "                                                                 \n",
            " classifier (Dense)          multiple                  1538      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 109,483,778\n",
            "Trainable params: 109,483,778\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2257: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "282/282 - 207s - loss: 0.5740 - accuracy: 0.7049 - val_loss: 0.4842 - val_accuracy: 0.7740 - 207s/epoch - 733ms/step\n",
            "Epoch 2/5\n",
            "282/282 - 183s - loss: 0.4597 - accuracy: 0.7853 - val_loss: 0.4520 - val_accuracy: 0.7880 - 183s/epoch - 650ms/step\n",
            "Epoch 3/5\n",
            "282/282 - 184s - loss: 0.4008 - accuracy: 0.8181 - val_loss: 0.4616 - val_accuracy: 0.7760 - 184s/epoch - 651ms/step\n",
            "Epoch 4/5\n",
            "282/282 - 184s - loss: 0.3412 - accuracy: 0.8503 - val_loss: 0.5019 - val_accuracy: 0.7540 - 184s/epoch - 651ms/step\n",
            "Epoch 5/5\n",
            "282/282 - 184s - loss: 0.2784 - accuracy: 0.8856 - val_loss: 0.5402 - val_accuracy: 0.7570 - 184s/epoch - 652ms/step\n",
            "\n",
            "######## Stereotype Prediction ########\n",
            "[[438 212]\n",
            " [149 201]] \n",
            " F1 score -  0.6175162714511391\n"
          ]
        }
      ],
      "source": [
        "Y_train_model = train_df['stereotype'].to_numpy()\n",
        "Y_test_model = test_labels[3].to_numpy()\n",
        "y_pred = try_model()\n",
        "print(\"\\n######## Stereotype Prediction ########\")\n",
        "print(confusion_matrix(Y_test_model, y_pred),\"\\n F1 score - \", f1_score(Y_test_model, y_pred, average='macro'))\n",
        "subtaskB += f1_score(Y_test_model, y_pred, average='macro')*(Y_test_model.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43ZAB6vodMCx",
        "outputId": "9e4a4219-4e95-4385-c1cf-78874e65f722"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"tf_bert_for_sequence_classification_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bert (TFBertMainLayer)      multiple                  109482240 \n",
            "                                                                 \n",
            " dropout_265 (Dropout)       multiple                  0         \n",
            "                                                                 \n",
            " classifier (Dense)          multiple                  1538      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 109,483,778\n",
            "Trainable params: 109,483,778\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2257: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "282/282 - 210s - loss: 0.5035 - accuracy: 0.7819 - val_loss: 0.4432 - val_accuracy: 0.8070 - 210s/epoch - 746ms/step\n",
            "Epoch 2/5\n",
            "282/282 - 184s - loss: 0.4152 - accuracy: 0.8164 - val_loss: 0.4163 - val_accuracy: 0.8240 - 184s/epoch - 653ms/step\n",
            "Epoch 3/5\n",
            "282/282 - 184s - loss: 0.3547 - accuracy: 0.8507 - val_loss: 0.4225 - val_accuracy: 0.8230 - 184s/epoch - 651ms/step\n",
            "Epoch 4/5\n",
            "282/282 - 183s - loss: 0.2956 - accuracy: 0.8836 - val_loss: 0.4506 - val_accuracy: 0.8180 - 183s/epoch - 651ms/step\n",
            "Epoch 5/5\n",
            "282/282 - 184s - loss: 0.2439 - accuracy: 0.9097 - val_loss: 0.4824 - val_accuracy: 0.8170 - 184s/epoch - 651ms/step\n",
            "\n",
            "######## Objectification Prediction ########\n",
            "[[568  84]\n",
            " [202 146]] \n",
            " F1 score -  0.6520325678049824\n"
          ]
        }
      ],
      "source": [
        "Y_train_model = train_df['objectification'].to_numpy()\n",
        "Y_test_model = test_labels[4].to_numpy()\n",
        "y_pred = try_model()\n",
        "print(\"\\n######## Objectification Prediction ########\")\n",
        "print(confusion_matrix(Y_test_model, y_pred),\"\\n F1 score - \", f1_score(Y_test_model, y_pred, average='macro'))\n",
        "subtaskB += f1_score(Y_test_model, y_pred, average='macro')*(Y_test_model.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FppcF3lAf5M0",
        "outputId": "195095cf-0fa8-4e70-ce72-fea362ba3d94"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"tf_bert_for_sequence_classification_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bert (TFBertMainLayer)      multiple                  109482240 \n",
            "                                                                 \n",
            " dropout_303 (Dropout)       multiple                  0         \n",
            "                                                                 \n",
            " classifier (Dense)          multiple                  1538      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 109,483,778\n",
            "Trainable params: 109,483,778\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2257: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "282/282 - 208s - loss: 0.3710 - accuracy: 0.8608 - val_loss: 0.3035 - val_accuracy: 0.9070 - 208s/epoch - 736ms/step\n",
            "Epoch 2/5\n",
            "282/282 - 184s - loss: 0.2890 - accuracy: 0.9055 - val_loss: 0.2879 - val_accuracy: 0.9070 - 184s/epoch - 652ms/step\n",
            "Epoch 3/5\n",
            "282/282 - 184s - loss: 0.2350 - accuracy: 0.9196 - val_loss: 0.2662 - val_accuracy: 0.9080 - 184s/epoch - 652ms/step\n",
            "Epoch 4/5\n",
            "282/282 - 184s - loss: 0.1865 - accuracy: 0.9369 - val_loss: 0.2733 - val_accuracy: 0.9150 - 184s/epoch - 654ms/step\n",
            "Epoch 5/5\n",
            "282/282 - 183s - loss: 0.1465 - accuracy: 0.9524 - val_loss: 0.2872 - val_accuracy: 0.9160 - 183s/epoch - 649ms/step\n",
            "\n",
            "######## Violence Prediction ########\n",
            "[[829  18]\n",
            " [110  43]] \n",
            " F1 score -  0.6651003129218951\n"
          ]
        }
      ],
      "source": [
        "Y_train_model = train_df['violence'].to_numpy()\n",
        "Y_test_model = test_labels[5].to_numpy()\n",
        "y_pred = try_model()\n",
        "print(\"\\n######## Violence Prediction ########\")\n",
        "print(confusion_matrix(Y_test_model, y_pred),\"\\n F1 score - \", f1_score(Y_test_model, y_pred, average='macro'))\n",
        "subtaskB += f1_score(Y_test_model, y_pred, average='macro')*(Y_test_model.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIlzjX4OmeHg",
        "outputId": "754555bb-959b-43e3-e047-876dc931d303"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Subtask A score - 0.6624040920716112 \n",
            " Subtask B score -  0.6476459139075553\n"
          ]
        }
      ],
      "source": [
        "print(\"Subtask A score -\", \n",
        "      subtaskA, \n",
        "      \"\\n Subtask B score - \", \n",
        "      subtaskB/(test_labels[5].sum()+ test_labels[4].sum()+ test_labels[3].sum()+test_labels[2].sum()+test_labels[1].sum()))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Bert_for_semeval.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
